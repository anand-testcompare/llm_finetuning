{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../docs/images/DSPy8.png\" alt=\"DSPy7 Image\" height=\"120\"/>\n",
    "\n",
    "### Multi-Agent DSPy Programs: Bootstrapping & Aggregating Multiple `ReAct` Agents\n",
    "\n",
    "This is a quick (somewhat advanced) example of DSPy. You're given a hard QA task and an agent architecture (`dspy.ReAct`), how do you get high scores without tinkering with prompts?\n",
    "\n",
    "There are many ways, but this notebook shows one complex strategy that DSPy makes near-trivial to achieve: we'll automatically bootstrap five different highly-effective prompts for ReAct, then optimize an aggregator that combines their powers.\n",
    "\n",
    "As is usually the case with DSPy, the code to do this is probably shorter than describing it in English, so let's jump right into that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) TLDR.\n",
    "\n",
    "We'll build a ReAct agent in DSPy that scores 30% accuracy on a retrieval-based question answering task.\n",
    "\n",
    "Then, we'll optimize it with `BootstrapFewShotWithRandomSearch` to get 46% accuracy.\n",
    "\n",
    "Then, we'll build a multi-agent aggregator over five different optimized versions of the agent.\n",
    "\n",
    "Our unoptimized aggregator will score 26%. It doesn't understand the task. Hence, we'll optimize the aggregator too.\n",
    "\n",
    "We'll end up with an optimized multi-agent system that scores a whopping 60% accuracy on the same task.\n",
    "\n",
    "The core portion of the code to do this can be fit into 10 lines of DSPy, but we'll sprinkle some short explanations below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Setting Up.\n",
    "\n",
    "We'll configure the language model (GPT-3.5) and the retrieval model (ColBERTv2 over Wikipedia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.datasets.hotpotqa import HotPotQA\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "gpt3 = dspy.OpenAI('gpt-3.5-turbo', max_tokens=4000)\n",
    "colbert = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "dspy.configure(lm=gpt3, rm=colbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Loading some data.\n",
    "\n",
    "We'll load 150 examples for training (`trainset`), 50 examples for validation & optimization (`valset`), and 300 examples for evaluation (`devset`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimethylpant/Development/llm_finetuning/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Example({'question': 'At My Window was released by which American singer-songwriter?', 'answer': 'John Townes Van Zandt'}) (input_keys={'question'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = HotPotQA(train_seed=1, train_size=200, eval_seed=2023, dev_size=300, test_size=0)\n",
    "trainset = [x.with_inputs('question') for x in dataset.train[0:150]]\n",
    "valset = [x.with_inputs('question') for x in dataset.train[150:200]]\n",
    "devset = [x.with_inputs('question') for x in dataset.dev]\n",
    "\n",
    "# show an example datapoint; it's just a question-answer pair\n",
    "trainset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) ReAct Agent.\n",
    "\n",
    "Our agent will just be a DSPy ReAct agent that takes a `question` and outputs the `answer` by using a ColBERTv2 retrieval tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = dspy.ReAct(\"question -> answer\", tools=[dspy.Retrieve(k=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate this **unoptimized** ReAct agent on the `devset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an evaluator on the first 300 examples of the devset.\n",
    "config = dict(num_threads=8, display_progress=True, display_table=5)\n",
    "evaluate = Evaluate(devset=devset, metric=dspy.evaluate.answer_exact_match, **config)\n",
    "\n",
    "evaluate(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Optimized ReAct.\n",
    "\n",
    "Let's use DSPy's simple `BootstrapFewShotWithRandomSearch` optimizer to create successful examples of the ReAct program and attempt to optimize the prompts using those constructed examples. In the future, we could try more sophisticated DSPy optimizers too, like `MIPRO`.\n",
    "\n",
    "We'll bootstrap 20 programs that way. Examples will be bootstrapped starting from the `trainset` and optimized over our tiny `valset`. We'll evaluate later on the `devset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(max_bootstrapped_demos=2, max_labeled_demos=0, num_candidate_programs=20, num_threads=8)\n",
    "tp = BootstrapFewShotWithRandomSearch(metric=dspy.evaluate.answer_exact_match, **config)\n",
    "optimized_react = tp.compile(agent, trainset=trainset, valset=valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 0.7 seconds after 2 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 42.1 seconds after 7 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 8.1 seconds after 7 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 62.5 seconds after 7 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 2.3 seconds after 3 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 10.7 seconds after 6 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}Backing off 4.6 seconds after 4 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 100.9 seconds after 8 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 8.2 seconds after 5 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 3.0 seconds after 3 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 4.2 seconds after 4 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 15.6 seconds after 5 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 50.0 seconds after 8 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 3.2 seconds after 3 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 6.9 seconds after 4 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 12.7 seconds after 5 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 12.2 seconds after 6 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n",
      "Backing off 67.4 seconds after 8 tries calling function <function GPT3.request at 0x712b80f00b80> with kwargs {}\n"
     ]
    }
   ],
   "source": [
    "evaluate(optimized_react)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Zero-Shot Aggregator.\n",
    "\n",
    "Let's now extract the best five bootstrapped ReAct programs. We'll build a simple DSPy aggregator that runs all of them then produces a final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsp.utils import flatten, deduplicate\n",
    "\n",
    "# the best-performing five ReAct programs from the optimization process\n",
    "AGENTS = [x[-1] for x in optimized_react.candidate_programs[:5]]\n",
    "\n",
    "class Aggregator(dspy.Module):\n",
    "\tdef __init__(self, temperature=0.0):\n",
    "\t\tself.aggregate = dspy.ChainOfThought('context, question -> answer')\n",
    "\t\tself.temperature = temperature\n",
    "\n",
    "\tdef forward(self, question):\n",
    "\t\t# Run all five agents with high temperature, then extract and deduplicate their observed contexts\n",
    "\t\twith dspy.context(lm=gpt3.copy(temperature=self.temperature)):\n",
    "\t\t\tpreds = [agent(question=question) for agent in AGENTS]\n",
    "\t\t\tcontext = deduplicate(flatten([flatten(p.observations) for p in preds]))\n",
    "\n",
    "\t\t# Run the aggregation step to produce a final answer\n",
    "\t\treturn self.aggregate(context=context, question=question)ValueError: not enough values to unpack (expected 2, got 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly evaluate the aggregator prior to optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator = Aggregator()\n",
    "evaluate(aggregator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Optimized Aggregator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(max_bootstrapped_demos=2, max_labeled_demos=6, num_candidate_programs=10, num_threads=8)\n",
    "tp = BootstrapFewShotWithRandomSearch(metric=dspy.evaluate.answer_exact_match, **kwargs)\n",
    "optimized_aggregator = tp.compile(aggregator, trainset=trainset, valset=valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_aggregator2 = optimized_aggregator.deepcopy()\n",
    "optimized_aggregator2.temperature = 0.7\n",
    "\n",
    "evaluate(optimized_aggregator2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Conclusion.\n",
    "\n",
    "Normally, we like to release notebooks with pre-computed caches and to inspect the prompts with `gpt3.inspect_history` to explore the behavior of optimization. See the intro notebook (or any of the Colab notebooks on the README) for such annotated examples!\n",
    "\n",
    "To keep the current release super quick, Omar will extend this notebook into an annotated version if there's significant interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Post-Conclusion Note.\n",
    "\n",
    "With a little bit of syntactic sugar, the main code in this notebook could be as short as 10 lines excluding whitespace:\n",
    "\n",
    "```python\n",
    "agent = dspy.ReAct(\"question -> answer\", tools=[dspy.Retrieve(k=1)])\n",
    "\n",
    "optimizer = BootstrapFewShotWithRandomSearch(metric=dspy.evaluate.answer_exact_match)\n",
    "optimized_react = optimizer.compile(agent, trainset=trainset, valset=valset)\n",
    "\n",
    "class Aggregator(dspy.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tself.aggregate = dspy.ChainOfThought('context, question -> answer')\n",
    "\n",
    "\tdef forward(self, question):\n",
    "        preds = [agent(question=question) for agent in optimized_react.best_programs[:5]]\n",
    "\t\treturn self.aggregate(context=deduplicate(flatten([p.observations for p in preds])), question=question)\n",
    "\t\n",
    "optimized_aggregator = optimizer.compile(aggregator, trainset=trainset, valset=valset)\n",
    "\n",
    "# Use it!\n",
    "optimized_aggregator(question=\"How many storeys are in the castle that David Gregory inherited?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
